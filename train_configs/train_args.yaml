## GPU settings
gpus: [0, 1] # it seems that gpus must be in [0, 1] instead of [1, 0]
distributed: False
# The following args used for distributed data parallelism
gpus_per_node: 2
total_nodes: 1

## working directory
work_dir: ./logs/train/temp/

## resume training
resume_path: null # read "None" in python
# resume_path: ./logs/train/alexnet_ddp/checkpoints/epoch_14_step_37545.pth

## optimisation settings
lr: 0.01
momentum: 0.9
weight_decay: 0.0005
nesterov: True

## training settings
num_epochs: 200
grad_accu_steps: 1
logging_interval: 500
saving_interval: 5

## data settings
data_root: ../datasets/imagenet/ILSVRC2015/Data/CLS-LOC/
num_workers: 16
# for training dataset
input_size: 224
color_jitter: [0.4, 0.4, 0.4]
resize_scale: [0.08, 1.0]
ratio: [0.75, 1.333333333]
interpolation: bilinear
horizontal_flip: True
mean: [0.485, 0.456, 0.406]
std: [0.229, 0.224, 0.225]
# for training dataloader
train_bs: 256 # for ddp it is equivalent to num_gpu times the number
# for testing dataset
test_rescales: [256]
test_bs: 512
